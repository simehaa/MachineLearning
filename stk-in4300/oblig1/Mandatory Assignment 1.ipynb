{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STK-IN4300, Simen HÃ¥pnes**\n",
    "\n",
    "This assignment with code might also be found at my **[github](https://github.com/simehaa/stk-in4300/tree/master/oblig1)**.\n",
    "\n",
    "# Exercise 1\n",
    "Contrast the ability of a lasso and a ridge regression model to predict the Duke CAD index (CADi) from the gene expression.\n",
    "\n",
    "## Algorithm\n",
    "The data file which is to be analysed contains 110 patients **with** Coronary Artery Disease.\n",
    "\n",
    "The X-data contains 22283 various gene expressions, $X\\in \\mathbb{R}^{110 \\times 22,283}$.\n",
    "\n",
    "The y-data contains the true CADi value , $y \\in \\mathbb{R}^{110 \\times 1}$.\n",
    "\n",
    "I have written a python code, which uses the packages from Scikit-learn to perform Ridge and LASSO regression. The script to solve the exercise is presented. I will provide explainatory information between the code blocks as well as comments in the actual code. \n",
    "\n",
    "To begin, I will do the necessary imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "import pandas as pd # for reading csv file\n",
    "import numpy as np # array/linear algebra library\n",
    "import os # convenient when reading/writing files to other directories\n",
    "from sklearn.linear_model import LassoCV, RidgeCV # Lasso and ridge with cross validation\n",
    "from sklearn.model_selection import train_test_split # simple split of train and test set\n",
    "from sklearn.preprocessing import scale # scaling of the data with both mean and std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of functions\n",
    "The two following functions are meant to easily read a file in the **./data/** directory and to plot/save figures to the **./figures/** directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filename):\n",
    "    \"\"\"Read a csv data file located in the ./data/ directory\n",
    "    Return design matrix X and y-data\"\"\"\n",
    "    DATA_DIR = \"./data\"\n",
    "    if not os.path.exists(DATA_DIR):\n",
    "        os.mkdir(DATA_DIR)\n",
    "    DATA_FILE = os.path.join(DATA_DIR,filename)\n",
    "    df = pd.read_csv(DATA_FILE).values\n",
    "    # 0th column contains file names\n",
    "    y = np.array(df[:,1]) # 1st column is y-data\n",
    "    X = np.array(df[:,2:]) # remaining columns is the X-data\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def save_fig(y_test, y_pred, score, fig_id):\n",
    "    \"\"\"Save a matplotlib figure to the ./figure/ directory.\n",
    "    If/else statements ask the user if they wish to overwrite an\n",
    "    already existing figure.\"\"\"\n",
    "    # Plot\n",
    "    l = y_test.shape[0]\n",
    "    x = np.linspace(1,l,l)\n",
    "    # plot of y_pred vs. y_test:\n",
    "    plt.plot(y_test, y_pred, 'ro', label=fr\"prediction, $R^2 = ${score:2.2f}\")\n",
    "    # plot of the ideal linear case\n",
    "    min = np.min(y_test)\n",
    "    max = np.max(y_test)\n",
    "    plt.plot([min,max],[min,max])\n",
    "    # plot options\n",
    "    plt.xlabel(r\"$y$\")\n",
    "    plt.ylabel(r\"$\\hat{y}$\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    # Saving of figure\n",
    "    FIGURE_DIR = \"./figures\"\n",
    "    if not os.path.exists(FIGURE_DIR):\n",
    "        os.mkdir(FIGURE_DIR)\n",
    "    FIGURE_FILE = os.path.join(FIGURE_DIR, fig_id)\n",
    "    if os.path.exists(FIGURE_FILE):\n",
    "        overwrite = str(input(\"The file \\\"\" + FIGURE_FILE + \\\n",
    "            \"\\\" already exists, do you wish to overwrite it [y/n]?\\n\"))\n",
    "        if overwrite == \"y\":\n",
    "            plt.savefig(FIGURE_FILE, format=\"png\")\n",
    "            plt.close()\n",
    "            print(\"Figure was overwritten.\")\n",
    "        else:\n",
    "            print(\"Figure was not saved.\")\n",
    "    else:\n",
    "        plt.savefig(FIGURE_FILE, format=\"png\")\n",
    "        plt.close()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of the main function is to use scikit-learn to perform Ridge regression and LASSO regression. \n",
    "\n",
    "Note that X and y are scaled so that both mean = 0 and std = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Info:\n",
    "    Perform Ridge and Lasso regression on gene data. Try to predict\n",
    "    The output which is the Duke CAD index\n",
    "    (Coronary Artery Disease index).\n",
    "\n",
    "    Notes:\n",
    "    Data file \"data_E1.csv\" contains the 110 pasients with case:\n",
    "    * The first column contains file IDs (not necessary for analysis)\n",
    "    * the second column contains the y data: true CADi\n",
    "    * the remaining columns contains 22283 features\n",
    "    \"\"\"\n",
    "    # Read data from file\n",
    "    X, y = read_file(\"data_E1.csv\")\n",
    "    # Scale data with both mean and std: e.g. y = y/std(y) - mean(y)\n",
    "    std = np.std(y)\n",
    "    mean = np.mean(y)\n",
    "    X = scale(X)\n",
    "    y = scale(y)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n",
    "    \n",
    "    # Ridge (fit_intercept=False assumes that data is already scaled)\n",
    "    clf_r = RidgeCV(fit_intercept=False).fit(X_train, y_train)\n",
    "    y_pred_r = clf_r.predict(X_test)\n",
    "    score_r = clf_r.score(X_test, y_test)\n",
    "    y_test_r = y_test*std + mean\n",
    "    y_pred_r = y_pred_r*std + mean\n",
    "    # Plot and save figure\n",
    "    save_fig(y_test_r, y_pred_r, score_r, \"ridge.png\")\n",
    "   \n",
    "    # Lasso (fit_intercept=False assumes that data is already scaled\n",
    "    # Note: cv=5 is the default of a future version of scikit-learn\n",
    "    clf_l = LassoCV(fit_intercept=False, cv=5).fit(X_train, y_train)\n",
    "    y_pred_l = clf_l.predict(X_test)\n",
    "    score_l = clf_l.score(X_test, y_test)\n",
    "    y_test_l = y_test*std + mean\n",
    "    y_pred_l = y_pred_l*std + mean\n",
    "    # Plot and save figure\n",
    "    save_fig(y_test_l, y_pred_l, score_l, \"lasso.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the main function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    np.random.seed(1) # affect the random splitting of train/test set: same seed obtain the same results each run\n",
    "    main() # call the main function where the regressions are performed\n",
    "\n",
    "# Test run, information about versions:\n",
    "# python 3.7.3\n",
    "# scikit-learn 0.21.3\n",
    "# numpy 1.17.2\n",
    "# pandas 0.25.1\n",
    "# matplotlib 3.1.1\n",
    "\"\"\"\n",
    "$ python main.py\n",
    "The file \"./figures/ridge.png\" already exists, do you wish to overwrite it [y/n]?\n",
    "y\n",
    "Figure was overwritten.\n",
    "The file \"./figures/lasso.png\" already exists, do you wish to overwrite it [y/n]?\n",
    "y\n",
    "Figure was overwritten.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "The blue lines in the figures shows the desired output if the models were perfect $\\hat{y} = y$. The red dots shows the model predictions: $\\hat{y}$ vs. $y$.\n",
    "\n",
    "### Ridge Regression\n",
    "\n",
    "![Ridge Regression](figures/ridge.png)\n",
    "\n",
    "### LASSO Regression\n",
    "\n",
    "![LASSO Regression](figures/lasso.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "The first notable feature which is shown is that Ridge regression provides a better $R^2$-score than the Lasso regression. \n",
    "\n",
    "Both models fail to predict values in the same range as the true CADi values. The true values are in the range $[20,100]$, whereas Ridge regression only provides predictions in the range of roughly $[38,59]$ and LASSO only provides predictions in the range of roughly $[44,52]$. \n",
    "\n",
    "Ridge regression has a higher variance of $\\hat{y}$ than LASSO regression and this might also be the problem with this model and this data set: Ridge regression seems to suffer from a too high variance. If this is true, the model complexity is too high, and the *training set* is overfitted, hence do these parameters not fit the *test set* very well. \n",
    "\n",
    "LASSO regression however has a much lower variance, and it seem to have a very small output range. In the figure with $\\hat{y}$ vs. $y$, the correlation is near linear, but not in the right direction. If the model was perfect, all the predictions would be linear and on top on the blue line (which is $\\hat{y}=y$). In this case the predictions appear almost at a flat line $\\hat{y} = const. + \\sigma$, and this is an indication that this model suffers from a high bias. If this is true, LASSO fails to find a correlation between the gene data and CADi by using this data set. It is also notable that the number of data points (110) might be too low compared to the number of features (22,283)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "Consider projection pursuit. Derive the expressions for $w$ which minimizes the linearized expression for the object function\n",
    "$$\n",
    "S = \\sum_{i=1}^N g'(w_{old}^T x_i)^2 \\left( \\frac{y_i - g(w_{old}^T x_i)}{g'(w_{old}^T x_i)} + w_{old}^T x_i - w^Tx_i \\right)^2\n",
    "$$\n",
    "In other words, we wish to minimize $S$ wrt. $w$. First, let's try to rewrite this expression to linear equation by getting rid of the summation sign. It is convenient to identify:\n",
    "* which of the parameters are scalars: $y_i$, $g$ and $g$.\n",
    "* which of the parameters are vectors: $x_i$, $w_{old}$ and $w$ must be vectors of length p.\n",
    "\n",
    "Now, lets rename some of the expressions in the sum, in order to simplify the problem:\n",
    "\n",
    "$$\n",
    "a_i = g'(w_{old}^T x_i)^2 \n",
    "$$\n",
    "\n",
    "$$\n",
    "b_i = \\frac{y_i - g(w_{old}^T x_i)}{g'(w_{old}^T x_i)} + w_{old}^T x_i\n",
    "$$\n",
    "\n",
    "Hence the equation is now\n",
    "$$\n",
    "S = \\sum_{i=1}^N  a_i \\left( b_i - w^Tx_i \\right)^2,\n",
    "$$\n",
    "\n",
    "The output must be a scalar, therefore\n",
    "\n",
    "$$\n",
    "S = \\sum_{i=1}^N  a_i \\left( b_i - w^Tx_i \\right)^2 = (b^T - w^TX^T) A (b^T - w^T X^T)^T\n",
    "$$\n",
    "\n",
    "where A is the diagonal matrix with the $a$ along its diagonal. The matrix $X$ now contains N rows of $x_i$. Further:\n",
    "\n",
    "$$\n",
    "(b^T - w^TX^T) A (b - Xw) = b^TAb - b^TAXw - w^TX^TAb + w^TX^TAXw\n",
    "$$\n",
    "\n",
    "\n",
    "Now lets confirm that all these terms are indeed scalars:\n",
    "\n",
    "$$\n",
    "b^TAb \\rightarrow (1\\times N)(N\\times N)(N \\times 1) \n",
    "$$\n",
    "$$\n",
    "b^TAXw \\rightarrow (1 \\times N)(N \\times N)(N \\times p)( p \\times 1)\n",
    "$$\n",
    "$$\n",
    "w^TX^TAb \\rightarrow (1\\times p)(p \\times N)(N \\times N)(N \\times 1)\n",
    "$$\n",
    "$$\n",
    "w^TX^TAXw \\rightarrow (1 \\times p)(p \\times N)(N \\times N)(N \\times p) (p \\times 1)\n",
    "$$\n",
    "\n",
    "\n",
    "Since all terms start and end with 1, the products are $(1\\times 1)$ which are scalars. \n",
    "\n",
    "Notice that the the second term, transposed is $(b^TAXw)^T = w^TX^TAb$ which is the third term. Since the transpose of a scalar yields the same scalar these two terms are identical. The expression is now\n",
    "\n",
    "$$\n",
    "S = b^TAb - 2b^TAXw + (Xw)^TAXw\n",
    "$$\n",
    "\n",
    "Now, to obtain the minimum, we wish to differentiate wrt. $w$ and set this equal to 0:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial }{\\partial w} \\left(b^TAb - 2b^TAXw + (Xw)^TAXw\\right) = 0\n",
    "$$\n",
    "$$\n",
    "- 2b^TAX + 2X^T A X w = 0\n",
    "$$\n",
    "\n",
    "Now, the first term is $(1\\times p)$ and the second term is $(p \\times n)$, I therefore transpose the first term to get the expressions on the same form:\n",
    "\n",
    "$$\n",
    "b^TAX = X^T A X w \n",
    "$$\n",
    "$$\n",
    "X^T A b = X^T A X w \n",
    "$$\n",
    "$$\n",
    "w = (X^TAX)^{-1} X^T A b\n",
    "$$\n",
    "\n",
    "Hence the solution is\n",
    "\n",
    "$$\n",
    "\\underset{w}{\\mathrm{argmin}} (S) = \\left(X^TAX\\right)^{-1}X^TAb\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
